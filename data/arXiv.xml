<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3AAI%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:AI&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/3eC8vfFF8FhY0s4CCMy1dHnVbKY</id>
  <updated>2025-09-18T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">42597</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2409.12922v1</id>
    <updated>2024-08-26T04:41:21Z</updated>
    <published>2024-08-26T04:41:21Z</published>
    <title>AI Thinking: A framework for rethinking artificial intelligence in
  practice</title>
    <summary>  Artificial intelligence is transforming the way we work with information
across disciplines and practical contexts. A growing range of disciplines are
now involved in studying, developing, and assessing the use of AI in practice,
but these disciplines often employ conflicting understandings of what AI is and
what is involved in its use. New, interdisciplinary approaches are needed to
bridge competing conceptualisations of AI in practice and help shape the future
of AI use. I propose a novel conceptual framework called AI Thinking, which
models key decisions and considerations involved in AI use across disciplinary
perspectives. The AI Thinking model addresses five practice-based competencies
involved in applying AI in context: motivating AI use in information processes,
formulating AI methods, assessing available tools and technologies, selecting
appropriate data, and situating AI in the sociotechnical contexts it is used
in. A hypothetical case study is provided to illustrate the application of AI
Thinking in practice. This article situates AI Thinking in broader
cross-disciplinary discourses of AI, including its connections to ongoing
discussions around AI literacy and AI-driven innovation. AI Thinking can help
to bridge divides between academic disciplines and diverse contexts of AI use,
and to reshape the future of AI in practice.
</summary>
    <author>
      <name>Denis Newman-Griffis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.12922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.12922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.11563v3</id>
    <updated>2024-07-26T09:52:15Z</updated>
    <published>2024-06-17T14:01:59Z</published>
    <title>Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI</title>
    <summary>  This perspective piece calls for the study of the new field of Intersymbolic
AI, by which we mean the combination of symbolic AI, whose building blocks have
inherent significance/meaning, with subsymbolic AI, whose entirety creates
significance/effect despite the fact that individual building blocks escape
meaning. Canonical kinds of symbolic AI are logic, games and planning.
Canonical kinds of subsymbolic AI are (un)supervised machine and reinforcement
learning. Intersymbolic AI interlinks the worlds of symbolic AI with its
compositional symbolic significance and meaning and of subsymbolic AI with its
summative significance or effect to enable culminations of insights from both
worlds by going between and across symbolic AI insights with subsymbolic AI
techniques that are being helped by symbolic AI principles. For example,
Intersymbolic AI may start with symbolic AI to understand a dynamic system,
continue with subsymbolic AI to learn its control, and end with symbolic AI to
safely use the outcome of the learned subsymbolic AI controller in the dynamic
system. The way Intersymbolic AI combines both symbolic and subsymbolic AI to
increase the effectiveness of AI compared to either kind of AI alone is likened
to the way that the combination of both conscious and subconscious thought
increases the effectiveness of human thought compared to either kind of thought
alone. Some successful contributions to the Intersymbolic AI paradigm are
surveyed here but many more are considered possible by advancing Intersymbolic
AI.
</summary>
    <author>
      <name>André Platzer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-75387-9_11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-75387-9_11" rel="related"/>
    <link href="http://arxiv.org/abs/2406.11563v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.11563v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01, 68T05, 68T07, 68T27, 68T30, 03B70" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.3; I.2.4; I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.07632v3</id>
    <updated>2024-04-17T18:37:12Z</updated>
    <published>2024-02-12T13:16:30Z</published>
    <title>Overconfident and Unconfident AI Hinder Human-AI Collaboration</title>
    <summary>  AI transparency is a central pillar of responsible AI deployment and
effective human-AI collaboration. A critical approach is communicating
uncertainty, such as displaying AI's confidence level, or its correctness
likelihood (CL), to users. However, these confidence levels are often
uncalibrated, either overestimating or underestimating actual CL, posing risks
and harms to human-AI collaboration. This study examines the effects of
uncalibrated AI confidence on users' trust in AI, AI advice adoption, and
collaboration outcomes. We further examined the impact of increased
transparency, achieved through trust calibration support, on these outcomes.
Our results reveal that uncalibrated AI confidence leads to both the misuse of
overconfident AI and disuse of unconfident AI, thereby hindering outcomes of
human-AI collaboration. Deficiency of trust calibration support exacerbates
this issue by making it harder to detect uncalibrated confidence, promoting
misuse and disuse of AI. Conversely, trust calibration support aids in
recognizing uncalibration and reducing misuse, but it also fosters distrust and
causes disuse of AI. Our findings highlight the importance of AI confidence
calibration for enhancing human-AI collaboration and suggest directions for AI
design and regulation.
</summary>
    <author>
      <name>Jingshu Li</name>
    </author>
    <author>
      <name>Yitian Yang</name>
    </author>
    <author>
      <name>Renwen Zhang</name>
    </author>
    <author>
      <name>Yi-chieh Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2402.07632v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.07632v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.05075v1</id>
    <updated>2022-11-09T18:07:10Z</updated>
    <published>2022-11-09T18:07:10Z</published>
    <title>Supporting AI/ML Security Workers through an Adversarial Techniques,
  Tools, and Common Knowledge (AI/ML ATT&amp;CK) Framework</title>
    <summary>  This paper focuses on supporting AI/ML Security Workers -- professionals
involved in the development and deployment of secure AI-enabled software
systems. It presents AI/ML Adversarial Techniques, Tools, and Common Knowledge
(AI/ML ATT&amp;CK) framework to enable AI/ML Security Workers intuitively to
explore offensive and defensive tactics.
</summary>
    <author>
      <name>Mohamad Fazelnia</name>
    </author>
    <author>
      <name>Ahmet Okutan</name>
    </author>
    <author>
      <name>Mehdi Mirakhorli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI/ML ATT&amp;CK</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.05075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.15481v2</id>
    <updated>2024-07-31T14:47:24Z</updated>
    <published>2024-03-21T03:44:59Z</published>
    <title>Navigating Fairness: Practitioners' Understanding, Challenges, and
  Strategies in AI/ML Development</title>
    <summary>  The rise in the use of AI/ML applications across industries has sparked more
discussions about the fairness of AI/ML in recent times. While prior research
on the fairness of AI/ML exists, there is a lack of empirical studies focused
on understanding the perspectives and experiences of AI practitioners in
developing a fair AI/ML system. Understanding AI practitioners' perspectives
and experiences on the fairness of AI/ML systems are important because they are
directly involved in its development and deployment and their insights can
offer valuable real-world perspectives on the challenges associated with
ensuring fairness in AI/ML systems. We conducted semi-structured interviews
with 22 AI practitioners to investigate their understanding of what a 'fair
AI/ML' is, the challenges they face in developing a fair AI/ML system, the
consequences of developing an unfair AI/ML system, and the strategies they
employ to ensure AI/ML system fairness. We developed a framework showcasing the
relationship between AI practitioners' understanding of 'fair AI/ML' system and
(i) their challenges in its development, (ii) the consequences of developing an
unfair AI/ML system, and (iii) strategies used to ensure AI/ML system fairness.
By exploring AI practitioners' perspectives and experiences, this study
provides actionable insights to enhance AI/ML fairness, which may promote
fairer systems, reduce bias, and foster public trust in AI technologies.
Additionally, we also identify areas for further investigation and offer
recommendations to aid AI practitioners and AI companies in navigating
fairness.
</summary>
    <author>
      <name>Aastha Pant</name>
    </author>
    <author>
      <name>Rashina Hoda</name>
    </author>
    <author>
      <name>Chakkrit Tantithamthavorn</name>
    </author>
    <author>
      <name>Burak Turhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, 8 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.15481v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.15481v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15294v1</id>
    <updated>2021-03-29T02:57:48Z</updated>
    <published>2021-03-29T02:57:48Z</published>
    <title>"Weak AI" is Likely to Never Become "Strong AI", So What is its Greatest
  Value for us?</title>
    <summary>  AI has surpassed humans across a variety of tasks such as image
classification, playing games (e.g., go, "Starcraft" and poker), and protein
structure prediction. However, at the same time, AI is also bearing serious
controversies. Many researchers argue that little substantial progress has been
made for AI in recent decades. In this paper, the author (1) explains why
controversies about AI exist; (2) discriminates two paradigms of AI research,
termed "weak AI" and "strong AI" (a.k.a. artificial general intelligence); (3)
clarifies how to judge which paradigm a research work should be classified
into; (4) discusses what is the greatest value of "weak AI" if it has no chance
to develop into "strong AI".
</summary>
    <author>
      <name>Bin Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.15294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.05551v1</id>
    <updated>2024-02-08T16:36:55Z</updated>
    <published>2024-02-08T16:36:55Z</published>
    <title>A Bibliometric View of AI Ethics Development</title>
    <summary>  Artificial Intelligence (AI) Ethics is a nascent yet critical research field.
Recent developments in generative AI and foundational models necessitate a
renewed look at the problem of AI Ethics. In this study, we perform a
bibliometric analysis of AI Ethics literature for the last 20 years based on
keyword search. Our study reveals a three-phase development in AI Ethics,
namely an incubation phase, making AI human-like machines phase, and making AI
human-centric machines phase. We conjecture that the next phase of AI ethics is
likely to focus on making AI more machine-like as AI matches or surpasses
humans intellectually, a term we coin as "machine-like human".
</summary>
    <author>
      <name>Di Kevin Gao</name>
    </author>
    <author>
      <name>Andrew Haverly</name>
    </author>
    <author>
      <name>Sudip Mittal</name>
    </author>
    <author>
      <name>Jingdao Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2403.05551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.05551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14527v1</id>
    <updated>2025-03-15T18:37:29Z</updated>
    <published>2025-03-15T18:37:29Z</published>
    <title>Threefold model for AI Readiness: A Case Study with Finnish Healthcare
  SMEs</title>
    <summary>  This study examines AI adoption among Finnish healthcare SMEs through
semi-structured interviews with six health-tech companies. We identify three AI
engagement categories: AI-curious (exploring AI), AI-embracing (integrating
AI), and AI-catering (providing AI solutions). Our proposed threefold model
highlights key adoption barriers, including regulatory complexities, technical
expertise gaps, and financial constraints. While SMEs recognize AI's potential,
most remain in early adoption stages. We provide actionable recommendations to
accelerate AI integration, focusing on regulatory reforms, talent development,
and inter-company collaboration, offering valuable insights for healthcare
organizations, policymakers, and researchers.
</summary>
    <author>
      <name>Mohammed Alnajjar</name>
    </author>
    <author>
      <name>Khalid Alnajjar</name>
    </author>
    <author>
      <name>Mika Hämäläinen</name>
    </author>
    <link href="http://arxiv.org/abs/2503.14527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.15922v1</id>
    <updated>2023-05-25T10:43:54Z</updated>
    <published>2023-05-25T10:43:54Z</published>
    <title>Towards a Capability Assessment Model for the Comprehension and Adoption
  of AI in Organisations</title>
    <summary>  The comprehension and adoption of Artificial Intelligence (AI) are beset with
practical and ethical problems. This article presents a 5-level AI Capability
Assessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to
assist practitioners in AI comprehension and adoption. These practical tools
were developed with business executives, technologists, and other
organisational stakeholders in mind. They are founded on a comprehensive
conception of AI compared to those in other AI adoption models and are also
open-source artefacts. Thus, the AI-CAM and AI-CM present an accessible
resource to help inform organisational decision-makers on the capability
requirements for (1) AI-based data analytics use cases based on machine
learning technologies; (2) Knowledge representation to engineer and represent
data, information and knowledge using semantic technologies; and (3) AI-based
solutions that seek to emulate human reasoning and decision-making. The AI-CAM
covers the core capability dimensions (business, data, technology,
organisation, AI skills, risks, and ethical considerations) required at the
five capability maturity levels to achieve optimal use of AI in organisations.
</summary>
    <author>
      <name> Butler</name>
    </author>
    <author>
      <name> Tom</name>
    </author>
    <author>
      <name> Espinoza-Limón</name>
    </author>
    <author>
      <name> Angelina</name>
    </author>
    <author>
      <name> Seppälä</name>
    </author>
    <author>
      <name> Selja</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of AI, Robotics &amp; Workplace Automation, 1 (1), 18-33
  (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2305.15922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.15922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.1; D.2.9; K.6.1; K.6.0; K.7.2; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.16424v1</id>
    <updated>2024-05-26T04:30:17Z</updated>
    <published>2024-05-26T04:30:17Z</published>
    <title>Improving Health Professionals' Onboarding with AI and XAI for
  Trustworthy Human-AI Collaborative Decision Making</title>
    <summary>  With advanced AI/ML, there has been growing research on explainable AI (XAI)
and studies on how humans interact with AI and XAI for effective human-AI
collaborative decision-making. However, we still have a lack of understanding
of how AI systems and XAI should be first presented to users without technical
backgrounds. In this paper, we present the findings of semi-structured
interviews with health professionals (n=12) and students (n=4) majoring in
medicine and health to study how to improve onboarding with AI and XAI. For the
interviews, we built upon human-AI interaction guidelines to create onboarding
materials of an AI system for stroke rehabilitation assessment and AI
explanations and introduce them to the participants. Our findings reveal that
beyond presenting traditional performance metrics on AI, participants desired
benchmark information, the practical benefits of AI, and interaction trials to
better contextualize AI performance, and refine the objectives and performance
of AI. Based on these findings, we highlight directions for improving
onboarding with AI and XAI and human-AI collaborative decision-making.
</summary>
    <author>
      <name>Min Hun Lee</name>
    </author>
    <author>
      <name>Silvana Xin Yi Choo</name>
    </author>
    <author>
      <name>Shamala D/O Thilarajah</name>
    </author>
    <link href="http://arxiv.org/abs/2405.16424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.16424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
